#!/usr/bin/env node

// Reads promptfoo JSON output and generates a markdown eval report.
// Usage: node format-report.js < output.json > report.md

const fs = require("fs");

const input = fs.readFileSync(process.stdin.fd, "utf-8");
const data = JSON.parse(input);

const results = data.results;
const stats = data.results.stats || {};

const totalTests = results.results.length;
const passed = results.results.filter((r) => r.success).length;
const failed = totalTests - passed;
const passRate = totalTests > 0 ? ((passed / totalTests) * 100).toFixed(0) : 0;

// Status emoji
const overallStatus = failed === 0 ? "✅" : "❌";
const statusBadge =
  failed === 0
    ? "![pass](https://img.shields.io/badge/evals-pass-brightgreen)"
    : `![fail](https://img.shields.io/badge/evals-${failed}%20failed-red)`;

let md = "";

// Header
md += `## ${overallStatus} Rewrite Eval Report\n\n`;
md += `${statusBadge}\n\n`;

// Summary table
md += `| Metric | Value |\n`;
md += `|--------|-------|\n`;
md += `| **Tests** | ${totalTests} |\n`;
md += `| **Passed** | ${passed} |\n`;
md += `| **Failed** | ${failed} |\n`;
md += `| **Pass Rate** | ${passRate}% |\n`;

if (stats.tokenUsage) {
  const totalTokens = stats.tokenUsage.total || 0;
  const cost = stats.tokenUsage.totalCost
    ? `$${stats.tokenUsage.totalCost.toFixed(4)}`
    : "n/a";
  md += `| **Tokens** | ${totalTokens.toLocaleString()} |\n`;
  md += `| **Cost** | ${cost} |\n`;
}

md += `\n`;

// Per-test details
md += `### Test Results\n\n`;
md += `<table>\n`;
md += `<tr><th>Status</th><th>Test</th><th>Details</th></tr>\n`;

for (const result of results.results) {
  const icon = result.success ? "✅" : "❌";
  const desc = escapeHtml(result.description || result.vars?.transcript?.slice(0, 60) || "—");

  // Build assertion details
  const assertionDetails = [];
  if (result.gradingResult?.componentResults) {
    for (const comp of result.gradingResult.componentResults) {
      const aIcon = comp.pass ? "✓" : "✗";
      const reason = escapeHtml(comp.reason || comp.assertion?.type || "—");
      assertionDetails.push(`<code>${aIcon}</code> ${reason}`);
    }
  }

  // Show output preview for failed tests
  let details = assertionDetails.join("<br>");
  if (!result.success && result.response?.output) {
    const preview = escapeHtml(result.response.output.slice(0, 120));
    details += `<br><br><b>Output:</b> <code>${preview}${result.response.output.length > 120 ? "…" : ""}</code>`;
  }

  md += `<tr>\n`;
  md += `<td>${icon}</td>\n`;
  md += `<td><b>${desc}</b></td>\n`;
  md += `<td>${details || "—"}</td>\n`;
  md += `</tr>\n`;
}

md += `</table>\n\n`;

// Footer with input/output samples for failed tests
const failures = results.results.filter((r) => !r.success);
if (failures.length > 0) {
  md += `### Failed Test Details\n\n`;
  for (const f of failures) {
    const desc = f.description || "Unnamed test";
    md += `<details>\n<summary>❌ ${escapeHtml(desc)}</summary>\n\n`;
    md += `**Input:**\n\`\`\`\n${f.vars?.transcript || "—"}\n\`\`\`\n\n`;
    md += `**Output:**\n\`\`\`\n${f.response?.output || "—"}\n\`\`\`\n\n`;

    if (f.gradingResult?.componentResults) {
      md += `**Assertions:**\n`;
      for (const comp of f.gradingResult.componentResults) {
        const icon = comp.pass ? "✅" : "❌";
        md += `- ${icon} ${comp.reason || comp.assertion?.type || "—"}\n`;
      }
    }
    md += `\n</details>\n\n`;
  }
}

// Metadata
md += `---\n`;
md += `<sub>Eval ID: ${results.id || "—"} · `;
md += `Provider: ${results.results[0]?.provider?.id || "—"} · `;
md += `Generated by <a href="https://promptfoo.dev">promptfoo</a></sub>\n`;

process.stdout.write(md);

function escapeHtml(str) {
  return str
    .replace(/&/g, "&amp;")
    .replace(/</g, "&lt;")
    .replace(/>/g, "&gt;")
    .replace(/"/g, "&quot;");
}
