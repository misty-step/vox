## Purpose
This directory contains a benchmarking suite designed to evaluate and compare the performance of various Large Language Models (LLMs) for transcript rewriting tasks. It automates the execution of a test corpus across multiple models via the OpenRouter API, measuring metrics such as latency, cost, and output quality. The suite generates detailed performance artifacts and provides model recommendations based on predefined quality targets and statistical distributions.

## Key Roles
- **`VoxBenchmarksMain`**: Serves as the program entry point, responsible for parsing configuration, loading the input corpus, and triggering the execution and reporting phases.
- **`BenchmarkRunner`**: Orchestrates the benchmarking process by iterating through models and corpus entries, recording individual invocations, and aggregating statistical summaries.
- **`OpenRouterBenchmarkClient`**: Manages communication with the OpenRouter API, handling request payload construction, network timeouts, and response decoding.
- **`BenchmarkReportMarkdown`**: Renders the final benchmark results into a formatted Markdown report, including methodology, result tables, and manual spot checks.
- **`BenchmarkConfig`**: Encapsulates execution parameters, including API authentication, model selections, and file paths for input and output.
- **`Distribution`**: Provides utility functions for calculating statistical metrics such as p50, p95, mean, and range for latency and cost data.

## Dependencies and Caveats
- Depends on `VoxCore`, `VoxPipeline`, and `VoxProviders` for processing levels, rewrite prompts, and quality gate evaluation.
- Requires a valid `OPENROUTER_API_KEY` provided via the environment.
- Communicates with the OpenRouter API at `https://openrouter.ai/api/v1/chat/completions`.
- Evaluation is constrained to specific processing levels: `.clean` and `.polish`.
- Quality assessment relies on `RewriteQualityGate` to determine if model outputs are acceptable.
- Includes built-in retry logic for throttled, timed-out, or network-related failures.
- Expects a versioned JSON corpus file containing entries with transcripts and associated processing levels.
- Model selection logic prioritizes quality pass rates, followed by p95 latency and mean cost.