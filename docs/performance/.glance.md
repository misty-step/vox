## Purpose
This directory defines the performance standards, measurement methodologies, and Service Level Objectives (SLOs) for the Vox dictation pipeline. It stores a standardized test corpus, comparative evaluations of third-party providers, and historical benchmark results used to select optimal models for speech-to-text and text-rewriting tasks.

## Key Roles
- **README.md**: Describes the two performance auditing lanes (pipeline overhead and live provider audits), CI reporting workflows, and the append-only artifact storage strategy.
- **latency-budget.md**: Establishes p50 and p95 latency targets for individual pipeline stages—encode, stt, rewrite, and paste—and defines the framework overhead budget.
- **rewrite-corpus.json**: Contains a versioned collection of test transcripts categorized by processing levels ("clean" and "polish") to ensure consistent benchmarking.
- **rewrite-model-lockdown-2026-02-23.md**: Documents the final selection of primary and fallback rewrite models based on competitive performance data.
- **rewrite-model-bakeoff-[date].md**: Summarizes comparative evaluations of candidate Large Language Models (LLMs) against quality, latency, and cost metrics.
- **stt-eval-[date].md**: Details the performance and accuracy metrics for Speech-to-Text providers including ElevenLabs, Deepgram, and OpenAI.

## Dependencies and Caveats
- **External Persistence**: Durable JSON performance artifacts are stored in a dedicated, append-only repository (`misty-step/vox-perf-audit`).
- **Provider Dependencies**: Real-world auditing relies on external APIs from ElevenLabs, Deepgram, OpenRouter, and Google Gemini.
- **Environment Constraints**: CI performance reporting requires a `PERF_AUDIT_TOKEN` secret; if missing, persistence steps are skipped.
- **Measurement Methodology**: Benchmarks distinguish between framework overhead (measured via mock providers) and end-to-end latency (measured via live providers).
- **SLO Scope**: Latency targets are specifically defined for short audio recordings of less than 30 seconds.
- **LLM Synthesis**: Performance reports optionally use OpenRouter (Gemini models) for actionable synthesis, failing open if the service is unavailable.
- **Measurement Artifacts**: Latency measurements include wall-clock request time and network overhead, while costs are derived from provider-reported metadata.
- **Routing Logic**: Production models may route through `ModelRoutedRewriteProvider` to direct APIs rather than through OpenRouter to reduce latency.